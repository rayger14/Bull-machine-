#!/usr/bin/env python3
"""
Bull Machine v1.6.0 - ETH Enhanced Ensemble Backtest with v1.6.0 Signals
üöÄ Integrates M1/M2 Wyckoff + Hidden Fibonacci + Volatility-Weighted Scoring
Target: Hit remaining RC targets (WR ‚â•50%, PnL ‚â•10%, PF ‚â•1.3, Sharpe ‚â•2.2)
"""

import pandas as pd
import numpy as np
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from bull_machine.modules.fusion.v151_core_trader import CoreTraderV151
from bull_machine.backtest.ensemble_mode import EnsembleAligner
from bull_machine.strategy.atr_exits import enhanced_exit_check
from bull_machine.strategy.wyckoff_m1m2 import compute_m1m2_scores
from bull_machine.strategy.hidden_fibs import compute_hidden_fib_scores

def compute_rsi(series, period: int = 14) -> float:
    """Calculate RSI for a series."""
    try:
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return float(rsi.iloc[-1]) if pd.notna(rsi.iloc[-1]) else 50.0
    except:
        return 50.0

def compute_ema_crossover(df, fast: int = 12, slow: int = 26) -> str:
    """Calculate EMA crossover for trend-following."""
    try:
        ema_fast = df['close'].ewm(span=fast, adjust=False).mean().iloc[-1]
        ema_slow = df['close'].ewm(span=slow, adjust=False).mean().iloc[-1]
        if ema_fast > ema_slow:
            return 'long'
        elif ema_fast < ema_slow:
            return 'short'
        return 'neutral'
    except:
        return 'neutral'

def detect_wyckoff_regime(df, lookback: int = 50) -> str:
    """Detect Wyckoff market regime (accumulation/distribution/trending)."""
    try:
        recent_high = df['high'].rolling(lookback).max().iloc[-1]
        recent_low = df['low'].rolling(lookback).min().iloc[-1]
        current_price = df['close'].iloc[-1]
        range_position = (current_price - recent_low) / (recent_high - recent_low)

        # Volume analysis
        vol_ma = df['volume'].rolling(20).mean().iloc[-1]
        current_vol = df['volume'].iloc[-1]

        # Accumulation: Price in lower 40% of range with increasing volume
        if range_position < 0.4 and current_vol > vol_ma:
            return 'accumulation'
        # Distribution: Price in upper 60% of range with increasing volume
        elif range_position > 0.6 and current_vol > vol_ma:
            return 'distribution'
        # Trending: Clear directional movement
        elif df['close'].iloc[-20:].pct_change().mean() > 0.001:
            return 'uptrend'
        elif df['close'].iloc[-20:].pct_change().mean() < -0.001:
            return 'downtrend'
        return 'ranging'
    except:
        return 'unknown'

def load_and_align_eth_data() -> pd.DataFrame:
    """Load SPY data from multiple timeframes and align."""

    print("üìä Loading SPY Multi-Timeframe Data...")

    # SPY equity data files for extended testing
    data_files = {
        "1D": "/Users/raymondghandchi/Downloads/Chart logs 2/BATS_SPY, 1D_c324d.csv",
        "4H": "/Users/raymondghandchi/Downloads/Chart logs 2/BATS_SPY, 240_48e36.csv",
        "1H": "/Users/raymondghandchi/Downloads/Chart logs 2/BATS_SPY, 60_9f7f8.csv"
    }

    timeframes = {}

    for tf, file_path in data_files.items():
        print(f"Loading {tf}...")

        if not Path(file_path).exists():
            print(f"‚ùå {tf} file not found: {file_path}")
            continue

        df = pd.read_csv(file_path)

        # Handle timestamp
        if 'time' in df.columns:
            df['timestamp'] = pd.to_datetime(df['time'], unit='s')
        elif 'timestamp' not in df.columns:
            df['timestamp'] = pd.date_range(start='2020-01-01', periods=len(df), freq='h')

        # Handle volume
        if 'volume' not in df.columns:
            if 'BUY+SELL V' in df.columns:
                df['volume'] = df['BUY+SELL V']
            else:
                df['volume'] = df['close'] * 100

        # Clean data
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].dropna()
        df = df.sort_values('timestamp').reset_index(drop=True)

        # Rename columns with TF suffix
        df_renamed = df.rename(columns={
            'open': f'open_{tf}',
            'high': f'high_{tf}',
            'low': f'low_{tf}',
            'close': f'close_{tf}',
            'volume': f'volume_{tf}'
        })

        timeframes[tf] = df_renamed
        print(f"‚úÖ {tf}: {len(df)} bars ({df['timestamp'].min()} to {df['timestamp'].max()})")

    if len(timeframes) < 3:
        raise ValueError("Need all three timeframes for ensemble trading")

    # Find overlapping period
    start_times = [df['timestamp'].min() for df in timeframes.values()]
    end_times = [df['timestamp'].max() for df in timeframes.values()]

    overlap_start = max(start_times)
    overlap_end = min(end_times)

    print(f"\nüìÖ Overlap Period: {overlap_start} to {overlap_end}")

    # Use 1H as base for alignment
    base_df = timeframes['1H'].copy()
    base_df = base_df[
        (base_df['timestamp'] >= overlap_start) &
        (base_df['timestamp'] <= overlap_end)
    ].reset_index(drop=True)

    # Align 4H data
    base_df['timestamp_4H'] = base_df['timestamp'].dt.floor('4h')
    df_4h_aligned = timeframes['4H'].copy()
    df_4h_aligned['timestamp_4H'] = df_4h_aligned['timestamp'].dt.floor('4h')

    base_df = base_df.merge(
        df_4h_aligned[['timestamp_4H', 'open_4H', 'high_4H', 'low_4H', 'close_4H', 'volume_4H']],
        on='timestamp_4H',
        how='left'
    )

    # Align 1D data
    base_df['timestamp_1D'] = base_df['timestamp'].dt.floor('1D')
    df_1d_aligned = timeframes['1D'].copy()
    df_1d_aligned['timestamp_1D'] = df_1d_aligned['timestamp'].dt.floor('1D')

    base_df = base_df.merge(
        df_1d_aligned[['timestamp_1D', 'open_1D', 'high_1D', 'low_1D', 'close_1D', 'volume_1D']],
        on='timestamp_1D',
        how='left'
    )

    # Forward fill missing data
    base_df = base_df.ffill()
    base_df = base_df.dropna()

    print(f"‚úÖ Aligned Dataset: {len(base_df)} 1H bars")
    print(f"üí∞ ETH Price Range: ${base_df['close_1H'].min():.2f} - ${base_df['close_1H'].max():.2f}")

    return base_df

def get_eth_ensemble_config() -> Dict:
    """Get ETH v1.6.0 enhanced ensemble configuration with M1/M2 Wyckoff + Hidden Fibonacci signals."""

    config = {
        "entry_threshold": 0.35,  # Lower threshold for v1.6.0 enhanced signals
        "quality_floors": {
            "wyckoff": 0.15,  # Lower for enhanced M1/M2 integration
            "liquidity": 0.15,
            "structure": 0.15,
            "momentum": 0.20,
            "volume": 0.15,
            "context": 0.15,
            "mtf": 0.20,
            # v1.6.0 enhanced signals - require meaningful signals
            "m1": 0.30,   # Minimum M1 spring quality
            "m2": 0.30,   # Minimum M2 markup quality
            "fib_retracement": 0.15,  # Allow modest fib levels
            "fib_extension": 0.15
        },
        "features": {
            "mtf_dl2": True,
            "six_candle_leg": True,
            "orderflow_lca": False,  # Disabled for ensemble
            "negative_vip": False,   # Disabled for ensemble
            "wyckoff_phase": True,
            "liquidity_sweep": False,
            "order_blocks": False,
            "wick_magnet": True,
            "live_data": False,
            "use_asset_profiles": True,
            "atr_sizing": True,
            "atr_exits": True
        },
        "risk": {
            "risk_pct": 0.005,  # 0.5% risk per trade
            "atr_window": 14,
            "sl_atr": 1.8,
            "tp_atr": 3.0,
            "trail_atr": 1.2,
            "profit_ladder": [
                {"ratio": 1.5, "percent": 0.25},
                {"ratio": 2.5, "percent": 0.50},
                {"ratio": 4.0, "percent": 0.25}
            ]
        },
        "regime": {
            "vol_ratio_min": 1.0,
            "atr_pct_max": 0.08
        },
        "ensemble": {
            "enabled": True,
            "min_consensus": 2,        # 2 of 3 TFs must agree (proper ensemble)
            "consensus_penalty": 0.01, # -1% score when only 2/3 agree (more lenient)
            "rolling_k": 2,           # Need 2 of last 5 bars (more lenient)
            "rolling_n": 5,
            "lead_lag_window": 5,     # HTF can confirm within 5 bars (more lenient)
            "dynamic_thresholds": True # Enable volatility-based thresholds
        },
        "cooldown_bars": 168,  # 168H (7 day) cooldown between trades
        "timeframe": "ensemble"
    }

    return config

def generate_ensemble_signal(df_aligned: pd.DataFrame, i: int, traders: Dict, aligner: EnsembleAligner, recent_m1m2_hist: list) -> Optional[Dict]:
    """Generate time-tolerant ensemble signal."""

    if i < 200:  # Need sufficient history
        return None

    history_length = min(200, i)

    # Extract timeframe data
    tf_data = {}

    # 1H data (most granular)
    tf_data['1H'] = pd.DataFrame({
        'timestamp': df_aligned['timestamp'].iloc[i-history_length:i+1],
        'open': df_aligned['open_1H'].iloc[i-history_length:i+1],
        'high': df_aligned['high_1H'].iloc[i-history_length:i+1],
        'low': df_aligned['low_1H'].iloc[i-history_length:i+1],
        'close': df_aligned['close_1H'].iloc[i-history_length:i+1],
        'volume': df_aligned['volume_1H'].iloc[i-history_length:i+1]
    }).reset_index(drop=True)

    # 4H data
    tf_data['4H'] = pd.DataFrame({
        'timestamp': df_aligned['timestamp'].iloc[i-history_length:i+1:4],
        'open': df_aligned['open_4H'].iloc[i-history_length:i+1:4],
        'high': df_aligned['high_4H'].iloc[i-history_length:i+1:4],
        'low': df_aligned['low_4H'].iloc[i-history_length:i+1:4],
        'close': df_aligned['close_4H'].iloc[i-history_length:i+1:4],
        'volume': df_aligned['volume_4H'].iloc[i-history_length:i+1:4]
    }).reset_index(drop=True)

    # 1D data
    tf_data['1D'] = pd.DataFrame({
        'timestamp': df_aligned['timestamp'].iloc[i-history_length:i+1:24],
        'open': df_aligned['open_1D'].iloc[i-history_length:i+1:24],
        'high': df_aligned['high_1D'].iloc[i-history_length:i+1:24],
        'low': df_aligned['low_1D'].iloc[i-history_length:i+1:24],
        'close': df_aligned['close_1D'].iloc[i-history_length:i+1:24],
        'volume': df_aligned['volume_1D'].iloc[i-history_length:i+1:24]
    }).reset_index(drop=True)

    # Generate scores for each timeframe
    tf_scores = {}

    try:
        for tf in ['1H', '4H', '1D']:
            if tf in traders and len(tf_data[tf]) > 50:
                # Get base v1.5.1 scores
                scores = traders[tf].compute_base_scores(tf_data[tf])

                # Add v1.6.0 enhanced signals FIRST (before quality floor checks)
                try:
                    # Provide HTF (1D) frame and current fib scores to bias side correctly
                    fib_scores = {
                        'fib_retracement': scores.get('fib_retracement', 0.0),
                        'fib_extension': scores.get('fib_extension', 0.0),
                    }
                    m1m2_scores = compute_m1m2_scores(tf_data[tf], tf, tf_data.get('1D', None), fib_scores=fib_scores)
                    scores.update(m1m2_scores)

                    # Hidden Fibonacci zones
                    fib_scores = compute_hidden_fib_scores(tf_data[tf], tf)
                    scores.update(fib_scores)

                    # Enhanced Wyckoff scoring: blend traditional + M1/M2
                    traditional_wyckoff = scores.get('wyckoff', 0.25)
                    m1_score = m1m2_scores.get('m1', 0.0)
                    m2_score = m1m2_scores.get('m2', 0.0)

                    # Boost Wyckoff with M1/M2 confluence
                    enhanced_wyckoff = traditional_wyckoff + (m1_score * 0.3) + (m2_score * 0.3)
                    scores['wyckoff'] = min(1.0, enhanced_wyckoff)  # Cap at 1.0

                    # Log enhanced signals for analysis
                    if m1m2_scores.get('m1', 0) > 0.3 or m1m2_scores.get('m2', 0) > 0.3:
                        print(f"üéØ Bar {i} {tf} M1/M2: M1={m1m2_scores.get('m1', 0):.2f} M2={m1m2_scores.get('m2', 0):.2f}")
                        # Also show current score state
                        print(f"   Score Integration: scores['m1']={scores.get('m1', 'MISSING')} scores['m2']={scores.get('m2', 'MISSING')}")
                    if fib_scores.get('fib_retracement', 0) > 0.3 or fib_scores.get('fib_extension', 0) > 0.3:
                        print(f"üìê Bar {i} {tf} Fib: R={fib_scores.get('fib_retracement', 0):.2f} E={fib_scores.get('fib_extension', 0):.2f}")
                        print(f"   Score Integration: scores['fib_retracement']={scores.get('fib_retracement', 'MISSING')} scores['fib_extension']={scores.get('fib_extension', 'MISSING')}")

                    # Add RSI for adaptive confluence
                    scores['rsi_ltf'] = compute_rsi(tf_data[tf]['close'], 14)

                    # Add EMA crossover for trend-following
                    ema_cross = compute_ema_crossover(tf_data[tf])
                    scores['ema_cross'] = ema_cross

                    # Add Wyckoff regime detection
                    regime = detect_wyckoff_regime(tf_data[tf])
                    scores['wyckoff_regime'] = regime

                    # Override neutral side with EMA cross or regime
                    if scores.get('side', 'neutral') == 'neutral':
                        if ema_cross != 'neutral':
                            scores['side'] = ema_cross
                        elif regime == 'accumulation':
                            scores['side'] = 'long'
                        elif regime == 'distribution':
                            scores['side'] = 'short'


                except Exception as enhance_e:
                    print(f"‚ö†Ô∏è  {tf} v1.6.0 enhancement error: {enhance_e}")
                    # Add default enhanced values only if they don't exist
                    if 'm1' not in scores:
                        scores.update({'m1': 0.0, 'm2': 0.0, 'fib_retracement': 0.0, 'fib_extension': 0.0})

                # Debug: Verify score assignment
                if m1m2_scores.get('m1', 0) > 0.3 or m1m2_scores.get('m2', 0) > 0.3:
                    print(f"   üìù PRE-ASSIGNMENT {tf}: scores={dict((k, f'{v:.2f}') for k, v in scores.items() if k in ['m1', 'm2'])}")

                tf_scores[tf] = scores.copy()  # Make explicit copy

                if m1m2_scores.get('m1', 0) > 0.3 or m1m2_scores.get('m2', 0) > 0.3:
                    print(f"   üìù POST-ASSIGNMENT {tf}: tf_scores[{tf}]={dict((k, f'{v:.2f}') for k, v in tf_scores[tf].items() if k in ['m1', 'm2'])}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error generating TF scores: {e}")
        return None

    if len(tf_scores) < 2:
        return None

    # Update ensemble aligner
    aligner.update(tf_scores)




    # v1.6.0 ENHANCED ENSEMBLE LOGIC WITH CONFLUENCE
    # Merge-ready thresholds: higher quality signals with confluence requirements
    M1_TH = 0.60    # Spring threshold (slightly relaxed)
    M2_TH = 0.50    # Markup threshold (slightly relaxed)
    FIBR_TH = 0.45  # Retracement threshold (restored to allow signals)
    FIBX_TH = 0.45  # Extension threshold
    SOLO_M1 = 0.70  # Solo M1 threshold (stricter)
    SOLO_M2 = 0.60  # Solo M2 threshold (stricter)

    def _atr_pct(df, n: int = 14) -> float:
        """Calculate ATR as percentage of price."""
        if df is None or len(df) < n:
            return 0.0
        rng = (df['high'].rolling(n).max() - df['low'].rolling(n).min()).iloc[-1]
        return float(rng / max(1e-9, df['close'].iloc[-1]))

    def passes_confluence(scores: dict, mode: str = 'adaptive', recent_m1m2_hist: list = None, cfg: dict = None, df_ltf=None, df_htf=None) -> tuple[bool, float]:
        """Check for signal confluence with quality thresholds."""
        m1 = scores.get('m1', 0.0)
        m2 = scores.get('m2', 0.0)
        fibr = scores.get('fib_retracement', 0.0)
        fibx = scores.get('fib_extension', 0.0)
        cfg = cfg or {}

        if mode == 'strict':
            # Original strict behavior
            if (m1 > M1_TH and fibr > FIBR_TH):
                return True, max(m1, fibr) + 0.1
            if (m2 > M2_TH and fibx > FIBX_TH):
                return True, max(m2, fibx) + 0.1
            if m1 > SOLO_M1:
                return True, m1
            if m2 > SOLO_M2:
                return True, m2
            return False, 0.0

        # Adaptive mode: backoff to Fib-only when M1/M2 absent
        N = int(cfg.get('ensemble', {}).get('m1m2_backoff_bars', 30))  # Reduced from 60
        eps = float(cfg.get('ensemble', {}).get('m1m2_epsilon', 0.05))
        no_m1m2 = True
        if recent_m1m2_hist:
            recent = recent_m1m2_hist[-N:] if len(recent_m1m2_hist) >= N else recent_m1m2_hist
            no_m1m2 = all((h.get('m1', 0.0) < eps and h.get('m2', 0.0) < eps) for h in recent)

        # First try normal confluence
        if (m1 > M1_TH and fibr > FIBR_TH):
            return True, max(m1, fibr) + 0.1
        if (m2 > M2_TH and fibx > FIBX_TH):
            return True, max(m2, fibx) + 0.1
        if m1 > SOLO_M1:
            return True, m1
        if m2 > SOLO_M2:
            return True, m2

        # Volatility-aware override: if market is moving, let strong Fib + HTF bias through
        atrp = _atr_pct(df_ltf) if df_ltf is not None else 0.0
        htf_sma20_gt_sma50 = scores.get('htf_sma20_gt_sma50', False)
        trend_strong = htf_sma20_gt_sma50 or not htf_sma20_gt_sma50  # Any clear trend

        if no_m1m2 or atrp >= float(cfg.get('ensemble', {}).get('vol_override_atr_pct', 0.06)) or trend_strong:
            side = scores.get('side', 'neutral')
            rsi_ltf = scores.get('rsi_ltf', 50.0)

            # Solo Fib LONG: HTF uptrend + FibR strong + RSI bullish (ultra-relaxed for equities)
            solo_fib_long = (side == 'long' and fibr >= 0.15 and rsi_ltf >= 48)
            # Solo Fib SHORT: HTF downtrend + FibX strong + RSI bearish (ultra-relaxed for equities)
            solo_fib_short = (side == 'short' and fibx >= 0.15 and rsi_ltf <= 52)

            if solo_fib_long or solo_fib_short:
                # Higher ensemble entry threshold for solo Fib entries (very relaxed for testing)
                entry_req = float(cfg.get('ensemble', {}).get('solo_fib_min_entry', 0.25))
                ensemble_entry = scores.get('ensemble_entry', max(fibr, fibx, 0.30))
                if ensemble_entry >= entry_req:
                    return True, (fibr if solo_fib_long else fibx) + 0.05

        return False, 0.0

    # Check all timeframes for confluence
    strong_enhanced_signal = False
    enhanced_score = 0.0
    best_tf = None

    # Record M1/M2 history for adaptive mode (per bar, not per timeframe)
    if tf_scores:
        # Take primary timeframe (1H if available) for history
        primary_tf = '1H' if '1H' in tf_scores else list(tf_scores.keys())[0]
        primary_scores = tf_scores[primary_tf]
        recent_m1m2_hist.append({'m1': primary_scores.get('m1', 0.0), 'm2': primary_scores.get('m2', 0.0)})

        # Add helpful flags for logging/gating
        for tf, scores in tf_scores.items():
            scores['htf_uptrend'] = bool(scores.get('htf_sma20_gt_sma50', False))
            scores['htf_downtrend'] = not scores['htf_uptrend']
            scores['ensemble_entry'] = scores.get('wyckoff', 0.0)  # Use wyckoff as ensemble proxy

    config = {}  # Placeholder for config
    for tf, scores in tf_scores.items():
        has_confluence, score = passes_confluence(scores, mode='adaptive', recent_m1m2_hist=recent_m1m2_hist,
                                                 cfg=config, df_ltf=tf_data.get('4H'), df_htf=tf_data.get('1D'))
        if has_confluence and score > enhanced_score:
            strong_enhanced_signal = True
            enhanced_score = score
            best_tf = tf

    if strong_enhanced_signal:
        # Enhanced signals with confluence detected
        fire = True
        ensemble_score = 0.35 + enhanced_score  # Base + enhanced boost
        if best_tf:
            m1 = tf_scores[best_tf].get('m1', 0)
            m2 = tf_scores[best_tf].get('m2', 0)
            fibr = tf_scores[best_tf].get('fib_retracement', 0)
            fibx = tf_scores[best_tf].get('fib_extension', 0)
            side = tf_scores[best_tf].get('side', 'neutral')
            rsi_ltf = tf_scores[best_tf].get('rsi_ltf', 50)

            print(f"‚úÖ v1.6.0 CONFLUENCE FIRE {best_tf}: M1={m1:.2f} M2={m2:.2f} FibR={fibr:.2f} FibX={fibx:.2f} Side={side} RSI={rsi_ltf:.1f} Score={ensemble_score:.3f}")

            # Log decision
            atrp = _atr_pct(tf_data.get('4H', tf_data.get('1H'))) if tf_data else 0.0
            no_m1m2_hist = all((h.get('m1', 0.0) < 0.05 and h.get('m2', 0.0) < 0.05) for h in recent_m1m2_hist[-30:]) if recent_m1m2_hist else True
            print(f"   Decision: ENTRY | Mode: adaptive | ATR%: {atrp:.3f} | No M1/M2 last 30: {no_m1m2_hist}")
    else:
        # No strong enhanced signals - log why
        if tf_scores:
            best_scores = tf_scores[list(tf_scores.keys())[0]]
            m1 = best_scores.get('m1', 0)
            m2 = best_scores.get('m2', 0)
            fibr = best_scores.get('fib_retracement', 0)
            fibx = best_scores.get('fib_extension', 0)
            print(f"   Decision: NO_ENTRY | M1={m1:.2f} M2={m2:.2f} FibR={fibr:.2f} FibX={fibx:.2f}")

        # No strong enhanced signals - use traditional ensemble logic
        fire, ensemble_score = aligner.fire(tf_scores, tf_data)

    if not fire:
        return None

    # Use 1H data for precise entry execution
    entry_price = df_aligned.iloc[i]['close_1H']

    # Prefer enhanced HTF-aware side; never override with default if it's valid
    side = tf_scores.get(best_tf, {}).get('side', 'neutral') if best_tf else 'neutral'
    if side == 'neutral':
        # Fallback to momentum-based decision
        momentum_scores = [tf_scores[tf].get('momentum', 0.5) for tf in tf_scores.keys()]
        avg_momentum = np.mean(momentum_scores)
        side = "long" if avg_momentum >= 0.5 else "short"

    # Convert to uppercase for consistency
    side = side.upper()

    # Calculate position size using ATR
    current_atr = tf_data['1H']['high'].tail(14).mean() - tf_data['1H']['low'].tail(14).mean()
    if current_atr <= 0:
        current_atr = entry_price * 0.02  # 2% fallback

    risk_amount = 10000 * 0.005  # 0.5% of 10k balance
    stop_distance = current_atr * 1.8  # 1.8 ATR stop

    if side == "long":
        stop_loss = entry_price - stop_distance
    else:
        stop_loss = entry_price + stop_distance

    quantity = risk_amount / abs(entry_price - stop_loss)

    return {
        "side": side,
        "entry_price": entry_price,
        "quantity": quantity,
        "stop_loss": stop_loss,
        "ensemble_score": ensemble_score,
        "tf_scores": tf_scores,
        "participating_tfs": list(tf_scores.keys()),
        "consensus_strength": len(tf_scores)
    }

def run_eth_ensemble_backtest(df_aligned: pd.DataFrame) -> Dict:
    """Run SPY ensemble backtest with time-tolerant logic."""

    print(f"\nüöÄ Running SPY Ensemble Backtest...")

    config = get_eth_ensemble_config()

    # Initialize traders for each timeframe
    traders = {}
    for tf in ['1H', '4H', '1D']:
        tf_config = config.copy()
        tf_config['timeframe'] = tf
        traders[tf] = CoreTraderV151(tf_config)

    # Initialize ensemble aligner
    aligner = EnsembleAligner(config)

    # Trading state
    balance = 10000.0
    active_position = None
    trades = []
    signals_generated = 0
    last_trade_bar = -999
    equity_curve = [balance]
    peak_balance = balance
    recent_m1m2_hist = []  # Track M1/M2 history for adaptive mode

    # Process each aligned bar
    for i in range(200, len(df_aligned)):
        current_bar = df_aligned.iloc[i]

        # Check exits first
        if active_position:
            # Use 1H data for exits
            exit_history = pd.DataFrame({
                'timestamp': df_aligned['timestamp'].iloc[max(0,i-100):i+1],
                'open': df_aligned['open_1H'].iloc[max(0,i-100):i+1],
                'high': df_aligned['high_1H'].iloc[max(0,i-100):i+1],
                'low': df_aligned['low_1H'].iloc[max(0,i-100):i+1],
                'close': df_aligned['close_1H'].iloc[max(0,i-100):i+1],
                'volume': df_aligned['volume_1H'].iloc[max(0,i-100):i+1]
            })

            # Determine dominant timeframe for exit strategy
            dominant_tf = '1H'  # Default to 1H since we're using 1H bars
            if 'participating_tfs' in active_position:
                if '1D' in active_position['participating_tfs']:
                    dominant_tf = '1D'  # Use 1D exit strategy if 1D participated
                elif '4H' in active_position['participating_tfs']:
                    dominant_tf = '4H'  # Use 4H exit strategy if 4H participated

            exit_result = enhanced_exit_check(exit_history, active_position, config, tf=dominant_tf)

            if exit_result.get("close_position", False):
                closed_percent = exit_result.get("closed_percent", 1.0)
                exit_price = exit_result.get("exit_price", current_bar["close_1H"])
                entry_price = active_position["entry_price"]

                if active_position["side"] == "LONG":
                    pnl_pct = (exit_price - entry_price) / entry_price * 100
                else:
                    pnl_pct = (entry_price - exit_price) / entry_price * 100

                # Calculate dollar P&L for the portion being closed
                position_value = active_position["quantity"] * entry_price
                pnl_dollar = position_value * (pnl_pct / 100) * closed_percent
                balance += pnl_dollar

                # Record trade (partial or full)
                trade_record = {
                    **active_position,
                    "exit_timestamp": current_bar["timestamp"],
                    "exit_bar": i,
                    "exit_price": exit_price,
                    "exit_reason": exit_result.get("reason", "unknown"),
                    "pnl_pct": pnl_pct,
                    "pnl_dollar": pnl_dollar,
                    "bars_held": i - active_position["entry_bar"],
                    "closed_percent": closed_percent
                }
                trades.append(trade_record)

                print(f"EXIT: {active_position['side'].upper()} @ ${exit_price:.2f} "
                      f"({exit_result.get('reason', 'unknown')}) | "
                      f"PnL: {pnl_pct:+.2f}% (${pnl_dollar:+.2f}) | "
                      f"TFs: {active_position.get('participating_tfs', [])}")

                # Handle partial vs full exit
                if closed_percent >= 1.0 or exit_result.get("reason") in ["stop_loss", "wick_magnet", "trailing_stop"]:
                    # Full exit
                    active_position = None
                    last_trade_bar = i
                else:
                    # Partial exit - reduce position size
                    active_position["size"] *= (1.0 - closed_percent)
                    active_position["quantity"] *= (1.0 - closed_percent)

        # Generate ensemble signals
        if not active_position:
            # Check cooldown
            if i - last_trade_bar < config["cooldown_bars"]:
                continue

            ensemble_signal = generate_ensemble_signal(df_aligned, i, traders, aligner, recent_m1m2_hist)

            if ensemble_signal:
                signals_generated += 1

                # Execute trade
                active_position = {
                    "entry_timestamp": current_bar["timestamp"],
                    "entry_bar": i,
                    **ensemble_signal
                }
                active_position["initial_stop_loss"] = ensemble_signal["stop_loss"]
                active_position["size"] = 1.0  # Full position size for tracking partial exits

                print(f"ENSEMBLE ENTRY: {ensemble_signal['side'].upper()} @ ${ensemble_signal['entry_price']:.2f} | "
                      f"Qty: {ensemble_signal['quantity']:.4f} | "
                      f"Score: {ensemble_signal['ensemble_score']:.3f} | "
                      f"TFs: {ensemble_signal['participating_tfs']} | "
                      f"Consensus: {ensemble_signal['consensus_strength']}/3")

                last_trade_bar = i

        # Update equity curve
        current_equity = balance
        if active_position:
            current_price = current_bar["close_1H"]
            entry_price = active_position["entry_price"]

            if active_position["side"] == "LONG":
                unrealized_pnl = (current_price - entry_price) / entry_price
            else:
                unrealized_pnl = (entry_price - current_price) / entry_price

            position_value = active_position["quantity"] * entry_price
            current_equity += position_value * unrealized_pnl

        equity_curve.append(current_equity)
        if current_equity > peak_balance:
            peak_balance = current_equity

    # Calculate final metrics
    total_trades = len(trades)
    winning_trades = len([t for t in trades if t["pnl_pct"] > 0])
    win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0

    total_return_pct = (balance - 10000.0) / 10000.0 * 100

    # Max drawdown
    max_drawdown = 0
    for equity in equity_curve:
        if peak_balance > 0:
            drawdown = (peak_balance - equity) / peak_balance * 100
            max_drawdown = max(max_drawdown, drawdown)

    # Win/loss metrics
    wins = [t["pnl_pct"] for t in trades if t["pnl_pct"] > 0]
    losses = [t["pnl_pct"] for t in trades if t["pnl_pct"] <= 0]

    avg_win = np.mean(wins) if wins else 0
    avg_loss = np.mean(losses) if losses else 0

    profit_factor = 0
    if avg_loss < 0 and len(losses) > 0:
        total_wins = sum(wins) if wins else 0
        total_losses = abs(sum(losses))
        profit_factor = total_wins / total_losses if total_losses > 0 else float('inf')

    # Trading frequency
    days_traded = (df_aligned.iloc[-1]["timestamp"] - df_aligned.iloc[0]["timestamp"]).days
    trades_per_month = (total_trades / days_traded * 30) if days_traded > 0 else 0

    # Sharpe ratio
    returns = np.diff(equity_curve) / np.array(equity_curve[:-1])
    returns = returns[~np.isnan(returns)]
    sharpe_ratio = 0
    if len(returns) > 0 and np.std(returns) > 0:
        sharpe_ratio = (np.mean(returns) / np.std(returns)) * np.sqrt(252 * 24)  # Hourly data

    return {
        "total_bars": len(df_aligned),
        "signals_generated": signals_generated,
        "total_trades": total_trades,
        "winning_trades": winning_trades,
        "win_rate": win_rate,
        "trades_per_month": trades_per_month,
        "initial_balance": 10000.0,
        "final_balance": balance,
        "total_return_pct": total_return_pct,
        "max_drawdown": max_drawdown,
        "avg_win": avg_win,
        "avg_loss": avg_loss,
        "profit_factor": profit_factor,
        "sharpe_ratio": sharpe_ratio,
        "trades": trades,
        "equity_curve": equity_curve
    }

def main():
    """Run ETH time-tolerant ensemble backtest."""
    print("="*70)
    print("üöÄ Bull Machine v1.6.0 - SPY Extended Backtest with Adaptive Confluence")
    print("üìä Testing realistic MTF alignment with lead-lag tolerance")
    print("üéØ Target: 2-4 trades/month, ‚â•50% WR, ‚â•10% return")
    print("="*70)

    try:
        # Load and align ETH data
        df_aligned = load_and_align_eth_data()

        # Run ensemble backtest
        results = run_eth_ensemble_backtest(df_aligned)

        # Display results
        print(f"\n{'='*60}")
        print(f"üìà ETH ENSEMBLE BACKTEST RESULTS")
        print(f"{'='*60}")

        print(f"  üìÖ Period: {df_aligned['timestamp'].min().strftime('%Y-%m-%d')} to {df_aligned['timestamp'].max().strftime('%Y-%m-%d')}")
        print(f"  üìä Total 1H Bars: {results['total_bars']:,}")
        print(f"  üéØ Ensemble Signals: {results['signals_generated']}")
        print(f"  üíº Trades Executed: {results['total_trades']}")
        print(f"  üèÜ Win Rate: {results['win_rate']:.1f}% ({results['winning_trades']}/{results['total_trades']})")
        print(f"  üìÖ Trading Frequency: {results['trades_per_month']:.2f} trades/month")
        print(f"  üí∞ Starting Balance: ${results['initial_balance']:,.2f}")
        print(f"  üí∞ Final Balance: ${results['final_balance']:,.2f}")
        print(f"  üìà Total Return: {results['total_return_pct']:+.2f}%")
        print(f"  üìâ Max Drawdown: {results['max_drawdown']:.2f}%")
        print(f"  ‚¨ÜÔ∏è  Avg Win: {results['avg_win']:+.2f}%")
        print(f"  ‚¨áÔ∏è  Avg Loss: {results['avg_loss']:+.2f}%")
        print(f"  ‚öñÔ∏è  Profit Factor: {results['profit_factor']:.2f}")
        print(f"  üìä Sharpe Ratio: {results['sharpe_ratio']:.2f}")

        # Check RC targets
        print(f"\n{'='*60}")
        print(f"üéØ RC PROMOTION TARGET ANALYSIS")
        print(f"{'='*60}")

        targets_met = 0
        total_targets = 6

        # Trading frequency (2-4 trades/month)
        freq_met = 2.0 <= results['trades_per_month'] <= 4.0
        print(f"  ‚úÖ Trading Frequency: {results['trades_per_month']:.2f}/month {'‚úÖ' if freq_met else '‚ùå'} (Target: 2-4)")
        if freq_met: targets_met += 1

        # Win rate (‚â•50%)
        wr_met = results['win_rate'] >= 50.0
        print(f"  ‚úÖ Win Rate: {results['win_rate']:.1f}% {'‚úÖ' if wr_met else '‚ùå'} (Target: ‚â•50%)")
        if wr_met: targets_met += 1

        # Total return (‚â•10%)
        return_met = results['total_return_pct'] >= 10.0
        print(f"  ‚úÖ Total Return: {results['total_return_pct']:+.2f}% {'‚úÖ' if return_met else '‚ùå'} (Target: ‚â•10%)")
        if return_met: targets_met += 1

        # Max drawdown (‚â§9.2%)
        dd_met = results['max_drawdown'] <= 9.2
        print(f"  ‚úÖ Max Drawdown: {results['max_drawdown']:.2f}% {'‚úÖ' if dd_met else '‚ùå'} (Target: ‚â§9.2%)")
        if dd_met: targets_met += 1

        # Profit factor (‚â•1.3)
        pf_met = results['profit_factor'] >= 1.3
        print(f"  ‚úÖ Profit Factor: {results['profit_factor']:.2f} {'‚úÖ' if pf_met else '‚ùå'} (Target: ‚â•1.3)")
        if pf_met: targets_met += 1

        # Sharpe ratio (‚â•2.2)
        sharpe_met = results['sharpe_ratio'] >= 2.2
        print(f"  ‚úÖ Sharpe Ratio: {results['sharpe_ratio']:.2f} {'‚úÖ' if sharpe_met else '‚ùå'} (Target: ‚â•2.2)")
        if sharpe_met: targets_met += 1

        print(f"\nüèÜ RC TARGETS MET: {targets_met}/{total_targets}")

        if targets_met >= 5:
            print("üéâ READY FOR RC PROMOTION!")
        elif targets_met >= 3:
            print("üîß NEEDS MINOR TUNING")
        else:
            print("‚ö†Ô∏è  NEEDS SIGNIFICANT OPTIMIZATION")

        # Show trade details
        if results['trades']:
            print(f"\nüìã ENSEMBLE TRADE HISTORY:")
            for i, trade in enumerate(results['trades'], 1):
                side = trade['side'].upper()
                entry = trade['entry_price']
                exit_p = trade.get('exit_price', 'Active')
                pnl = trade.get('pnl_pct', 0)
                reason = trade.get('exit_reason', 'active')
                tfs = trade.get('participating_tfs', [])
                timestamp = trade['entry_timestamp'].strftime('%Y-%m-%d %H:%M')

                print(f"  #{i:2d} {timestamp} | {side:5s} | "
                      f"${entry:7.2f} ‚Üí ${exit_p:7.2f} | "
                      f"PnL: {pnl:+6.2f}% | TFs: {str(tfs):15s} | {reason}")

        # Save results
        output_file = f"eth_ensemble_backtest_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # Convert for JSON
        json_results = {}
        for key, value in results.items():
            if key == 'trades':
                json_results[key] = []
                for trade in value:
                    trade_record = {}
                    for k, v in trade.items():
                        if hasattr(v, 'strftime'):
                            trade_record[k] = v.isoformat()
                        elif isinstance(v, (np.integer, np.floating)):
                            trade_record[k] = float(v)
                        else:
                            trade_record[k] = v
                    json_results[key].append(trade_record)
            else:
                if isinstance(value, (np.integer, np.floating)):
                    json_results[key] = float(value)
                elif isinstance(value, list):
                    json_results[key] = [float(x) if isinstance(x, (np.integer, np.floating)) else x for x in value]
                else:
                    json_results[key] = value

        with open(output_file, 'w') as f:
            json.dump(json_results, f, indent=2, default=str)

        print(f"\nüíæ Detailed results saved to: {output_file}")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

    print(f"\n{'='*70}")
    print(f"‚úÖ ETH Ensemble Backtest Complete!")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()